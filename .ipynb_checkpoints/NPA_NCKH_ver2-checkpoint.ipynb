{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "laden-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "circular-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "suburban-meditation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>articleID</th>\n",
       "      <th>sapo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1002681074</td>\n",
       "      <td>4243622</td>\n",
       "      <td>Galaxy S21 Ultra được trang bị pin 5.000 mAh n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002681074</td>\n",
       "      <td>4243459</td>\n",
       "      <td>Thấy nòng súng súng thò ra khỏi cửa, nữ cảnh s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002681074</td>\n",
       "      <td>4238172</td>\n",
       "      <td>Sau khi có chức vô địch Australia Mở rộng, Nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002681074</td>\n",
       "      <td>4237883</td>\n",
       "      <td>Novak Djokovic đạt tỷ lệ thắng 81% trước các đ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002681074</td>\n",
       "      <td>4231206</td>\n",
       "      <td>Một người không thể đi hơn một xe máy ra đường...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2726</th>\n",
       "      <td>1003347668</td>\n",
       "      <td>4154347</td>\n",
       "      <td>Trên chương trình El Larguero của đài Cadena S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2727</th>\n",
       "      <td>1003347668</td>\n",
       "      <td>4154384</td>\n",
       "      <td>Tiền đạo Robert Lewandowski nói anh xứng đáng ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2728</th>\n",
       "      <td>1003347668</td>\n",
       "      <td>4155165</td>\n",
       "      <td>Hầu hết những chỉ số tấn công của Lionel Messi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2729</th>\n",
       "      <td>1003347668</td>\n",
       "      <td>4155219</td>\n",
       "      <td>Indonesia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>1003347668</td>\n",
       "      <td>4155786</td>\n",
       "      <td>Nếu Lionel Messi gia nhập Man City, những ngườ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2617 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             uid  articleID                                               sapo\n",
       "0     1002681074    4243622  Galaxy S21 Ultra được trang bị pin 5.000 mAh n...\n",
       "1     1002681074    4243459  Thấy nòng súng súng thò ra khỏi cửa, nữ cảnh s...\n",
       "2     1002681074    4238172  Sau khi có chức vô địch Australia Mở rộng, Nov...\n",
       "3     1002681074    4237883  Novak Djokovic đạt tỷ lệ thắng 81% trước các đ...\n",
       "4     1002681074    4231206  Một người không thể đi hơn một xe máy ra đường...\n",
       "...          ...        ...                                                ...\n",
       "2726  1003347668    4154347  Trên chương trình El Larguero của đài Cadena S...\n",
       "2727  1003347668    4154384  Tiền đạo Robert Lewandowski nói anh xứng đáng ...\n",
       "2728  1003347668    4155165  Hầu hết những chỉ số tấn công của Lionel Messi...\n",
       "2729  1003347668    4155219                                          Indonesia\n",
       "2730  1003347668    4155786  Nếu Lionel Messi gia nhập Man City, những ngườ...\n",
       "\n",
       "[2617 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/traindata_demo.csv')\n",
    "df.dropna(subset=['sapo'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-environment",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "micro-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lấy user mà clicked articleID => article khác của user khác\n",
    "def get_negative_sample(articleID, df, npratio=4):\n",
    "    uid = df[df.articleID==articleID].uid.values[0]\n",
    "    tmp_df = df[df.uid != uid].sample(4)\n",
    "    return [articleID] + list(tmp_df.articleID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "diverse-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_clicked(df, npratio=4):\n",
    "    uids = df.uid.unique()\n",
    "    user_count = len(uids)\n",
    "    userid_dict = {}\n",
    "    for uid in uids:\n",
    "        if uid not in userid_dict:\n",
    "            userid_dict[uid] = len(userid_dict) # map uid_raw -> 0 1 2\n",
    "\n",
    "    all_train_id = []\n",
    "    all_train_pn = []\n",
    "    all_label = []\n",
    "    \n",
    "    all_test_id = []\n",
    "    all_test_pn = []\n",
    "    all_test_label = []\n",
    "    all_test_index = []\n",
    "    \n",
    "    all_user_pos = []\n",
    "    all_test_user_pos = []\n",
    "\n",
    "    for uid in uids:\n",
    "        tmp_df = df[df.uid==uid] # df of uid\n",
    "        clicked_news = tmp_df.articleID.values\n",
    "        clicked_news = set(clicked_news)  # get all unique article which user clicked\n",
    "        \n",
    "        for idx in range(len(tmp_df)-1):\n",
    "            line = tmp_df.iloc[idx]\n",
    "            all_train_pn.append(get_negative_sample(line.articleID, df))\n",
    "            all_label.append([1,0,0,0,0])\n",
    "            all_train_id.append(userid_dict[uid])\n",
    "\n",
    "            remain_clicked = list(clicked_news - set([line.articleID]))\n",
    "            remain_clicked = random.sample(remain_clicked, min(50, len(remain_clicked)))\n",
    "            remain_clicked += [0] * (50-len(remain_clicked)) # <50 cho = 0\n",
    "            all_user_pos.append(remain_clicked)\n",
    "        \n",
    "        # get the last line for testing\n",
    "        sess_index = []\n",
    "        sess_index.append(len(all_test_pn))\n",
    "\n",
    "        line = tmp_df.iloc[-1]\n",
    "        all_test_pn += get_negative_sample(line.articleID, df)\n",
    "        sess_index.append(len(all_test_pn))\n",
    "        all_test_index.append(sess_index)\n",
    "        all_test_label += [1,0,0,0,0]\n",
    "        all_test_id += [userid_dict[uid]] * (npratio+1)\n",
    "        allpos = random.sample(clicked_news, min(50, len(clicked_news)))\n",
    "        allpos += [0] * (50-len(allpos))\n",
    "        for i in range(5):\n",
    "            all_test_user_pos.append(allpos)\n",
    "    \n",
    "    all_train_pn = np.array(all_train_pn,dtype='int32')\n",
    "    all_label = np.array(all_label,dtype='int32')\n",
    "    all_train_id = np.array(all_train_id,dtype='uint64')\n",
    "    all_test_pn = np.array(all_test_pn,dtype='int32')\n",
    "    all_test_label = np.array(all_test_label,dtype='int32')\n",
    "    all_test_id = np.array(all_test_id,dtype='uint64')\n",
    "    all_user_pos = np.array(all_user_pos,dtype='int32')\n",
    "    all_test_user_pos = np.array(all_test_user_pos, dtype='int32')\n",
    "\n",
    "    return (userid_dict, user_count, all_train_pn, all_label, all_train_id, all_test_pn, all_test_label, all_test_id, all_user_pos, all_test_user_pos, all_test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "userid_dict,user_count, all_train_pn, all_label, all_train_id, all_test_pn, all_test_label, all_test_id, all_user_pos, all_test_user_pos, all_test_index = preprocess_clicked(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-testimony",
   "metadata": {},
   "source": [
    "## Tokenizer and make word_dict of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "injured-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "VnCoreNLP_jar_file = '../../vncorenlp/VnCoreNLP-1.1.1.jar'\n",
    "rdrsegmenter = VnCoreNLP(VnCoreNLP_jar_file, annotators='wseg')\n",
    "embedding_dim=768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "activated-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_news(df):\n",
    "    sapos = df.sapo.values\n",
    "    articleIds = df.articleID.values\n",
    "\n",
    "    news = {} \n",
    "\n",
    "    for i in range(len(articleIds)):\n",
    "        if articleIds[i] not in news:\n",
    "            tokenized_words = rdrsegmenter.tokenize(sapos[i])[0]\n",
    "            news[articleIds[i]] = tokenized_words\n",
    "\n",
    "    \n",
    "    word_dict_raw = {'PADDING': [0,999999]}\n",
    "    for articleId in news:\n",
    "        for word in news[articleId]:\n",
    "            if word in word_dict_raw:\n",
    "                word_dict_raw[word][1] += 1 # increase freq\n",
    "            else:\n",
    "                word_dict_raw[word] = [len(word_dict_raw), 1] # format: [index, freq]\n",
    "                \n",
    "    word_dict = {}\n",
    "    for i in word_dict_raw:\n",
    "        if word_dict_raw[i][1] >= 2:\n",
    "            word_dict[i] = [len(word_dict), word_dict_raw[i][1]]\n",
    "    print('len word_dict (freq>=2 vs raw):', len(word_dict), len(word_dict_raw)) # chỉ để so sánh (loại bỏ freq =1)\n",
    "    \n",
    "    print('leng news (tokenizer):',len(news))\n",
    "\n",
    "    news_words = [ [0]*30 ] # \n",
    "    news_index = {0:0}\n",
    "    \n",
    "    for articleId in news: # quét các article\n",
    "        word_id = []\n",
    "        news_index[articleId] = len(news_index)\n",
    "        for word in news[articleId]: # quét các tokens\n",
    "            if word in word_dict:\n",
    "                word_id.append(word_dict[word][0])\n",
    "        word_id = word_id[:30] # lấy word_id của article (embedd)\n",
    "        news_words.append(word_id + [0]*(30-len(word_id))) # max 30 tokens, <30 cho =0\n",
    "    \n",
    "    news_words = np.array(news_words, dtype='int32')\n",
    "\n",
    "    return word_dict, news_words, news_index, news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afraid-oxide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len word_dict (freq>=2 vs raw): 3111 7097\n",
      "leng news (tokenizer): 2279\n",
      "CPU times: user 5.22 s, sys: 305 ms, total: 5.52 s\n",
      "Wall time: 9.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_dict, news_words, news_index, news = preprocess_news(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-living",
   "metadata": {},
   "source": [
    "## Phobert + embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "certain-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "# from transformers import RobertaConfig, RobertaModel\n",
    "\n",
    "from fairseq.data.encoders.fastbpe import fastBPE\n",
    "from fairseq.data import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "qualified-vault",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../PhoBERT_base_transformers/dict.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ddde5278263d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../PhoBERT_base_transformers/dict.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.6/site-packages/fairseq/data/dictionary.py\u001b[0m in \u001b[0;36madd_from_file\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfnfe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mfnfe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 raise Exception(\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.6/site-packages/fairseq/data/dictionary.py\u001b[0m in \u001b[0;36madd_from_file\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPathManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfnfe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../PhoBERT_base_transformers/dict.txt'"
     ]
    }
   ],
   "source": [
    "vocab = Dictionary()\n",
    "vocab.add_from_file('../PhoBERT_base_transformers/dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bert model\n",
    "from fairseq.models.roberta import RobertaModel\n",
    "import os\n",
    "phobert = RobertaModel.from_pretrained('../PhoBERT_base_fairseq', checkpoint_file='model.pt')\n",
    "phobert.eval()  # disable dropout (or leave in train mode to finetune)\n",
    "\n",
    "# Incorporate the BPE encoder into PhoBERT-base \n",
    "from fairseq.data.encoders.fastbpe import fastBPE  \n",
    "from fairseq import options  \n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--bpe-codes', \n",
    "    default=\"../PhoBERT_base_transformers/bpe.codes\",\n",
    "    required=False,\n",
    "    type=str,\n",
    "    help='path to fastBPE BPE'\n",
    ")\n",
    "args, unknown = parser.parse_known_args()\n",
    "phobert.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "focused-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word_dict):\n",
    "    embedding_dict = {}\n",
    "\n",
    "    for word in word_dict:\n",
    "        input_ids = vocab.encode_line(word, append_eos=False, add_if_not_exist=False).long()\n",
    "        embedding_tensor = phobert.extract_features(input_ids)\n",
    "        embedding_dict[word] = embedding_tensor.data.cpu().numpy()[0][0]\n",
    "        \n",
    "    embedding_matrix = [0]*len(word_dict)\n",
    "    cand = []\n",
    "\n",
    "    for i in embedding_dict:\n",
    "        embedding_matrix[word_dict[i][0]] = np.array(embedding_dict[i], dtype='float32')\n",
    "        cand.append(embedding_matrix[word_dict[i][0]])\n",
    "    \n",
    "    cand = np.array(cand, dtype='float32')\n",
    "    mu = np.mean(cand, axis=0)\n",
    "    Sigma = np.cov(cand.T)\n",
    "    norm = np.random.multivariate_normal(mu, Sigma, 1)\n",
    "\n",
    "    for i in range(len(embedding_matrix)):\n",
    "        if type(embedding_matrix[i]) == int: # unknown words\n",
    "            embedding_matrix[i] = np.reshape(norm, embedding_dim)\n",
    "    \n",
    "    embedding_matrix[0] = np.zeros(embedding_dim, dtype='float32')\n",
    "    embedding_matrix = np.array(embedding_matrix, dtype='float32')\n",
    "\n",
    "    print(embedding_matrix.shape)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embedding_mat = get_embedding(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-combining",
   "metadata": {},
   "source": [
    "## LOAD DATA AND MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "recorded-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load thuytt_ver2\n",
    "working_dir = '/home/thuytt/test_bert/NCKH/model/thuytt_ver2/'\n",
    "os.chdir(working_dir)\n",
    "\n",
    "file = open('dataloader.pkl', 'rb')\n",
    "num_users, all_train_pn, all_label, all_train_id, all_test_pn, all_test_label, all_test_id, all_user_pos, all_test_user_pos, all_test_index = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open('phobert_news_preprocess.pkl', 'rb')\n",
    "word_dict, news_words, news_index, news = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open('phobert_embed_mat.pkl', 'rb')\n",
    "embedding_mat = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-attempt",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "centered-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-apparatus",
   "metadata": {},
   "source": [
    "## Generate batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "quick-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hàm để convert articles word sang dạng index dựa vào từ điển word_dict\n",
    "def _articles_to_index(arr_np):\n",
    "    arr_index_articles = []\n",
    "    for arr_articles in arr_np:\n",
    "        index_article = []\n",
    "        for article in arr_articles:\n",
    "            index_article.append(news_index[article])\n",
    "        arr_index_articles.append(index_article)\n",
    "    arr_index_articles = np.array(arr_index_articles, dtype='int32')\n",
    "    return arr_index_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "scheduled-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data_train(all_train_pn,all_label,all_train_id,all_user_pos,batch_size):\n",
    "    inputid = np.arange(len(all_label))\n",
    "    np.random.shuffle(inputid)\n",
    "    y=all_label\n",
    "    batches = [inputid[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            if(i.size ==0):\n",
    "                continue\n",
    "            index_all_train_pn = _articles_to_index(all_train_pn[i])\n",
    "            candidate = news_words[index_all_train_pn]\n",
    "            candidate_split=[candidate[:,k,:] for k in range(candidate.shape[1])]\n",
    "\n",
    "            #\n",
    "            index_all_user_pos = _articles_to_index(all_user_pos[i])\n",
    "            browsed_news=news_words[index_all_user_pos]\n",
    "            browsed_news_split=[browsed_news[:,k,:] for k in range(browsed_news.shape[1])]\n",
    "            userid=np.expand_dims(all_train_id[i],axis=1)\n",
    "            label=all_label[i]\n",
    "            yield (candidate_split + browsed_news_split + [userid], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fewer-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data_test(all_test_pn,all_test_label,all_test_id,all_test_user_pos,batch_size):\n",
    "    inputid = np.arange(len(all_test_label))\n",
    "    y=all_test_label\n",
    "    batches = [inputid[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            if(i.size ==0):\n",
    "                continue\n",
    "            index_all_test_pn = [news_index[x] for x in all_test_pn[i]]\n",
    "            candidate = news_words[index_all_test_pn]\n",
    "\n",
    "            browsed_news=news_words[_articles_to_index(all_test_user_pos[i])]\n",
    "            browsed_news_split=[browsed_news[:,k,:] for k in range(browsed_news.shape[1])]\n",
    "            userid=np.expand_dims(all_test_id[i],axis=1)\n",
    "            label=all_test_label[i]\n",
    "\n",
    "            yield ([candidate] + browsed_news_split + [userid], label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-links",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "equivalent-candy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "vocal-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH=30 # số lượng tokens cho mỗi article\n",
    "MAX_SENTS=50 # maximum clicked article for user embedding\n",
    "npratio=4\n",
    "batch_size=32\n",
    "n_epoch=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed theo code của a hiếu, e cũng chưa biết để làm gì nên cứ kệ để đó\n",
    "def seed_everything(SEED):\n",
    "    np.random.seed(SEED)\n",
    "    tf.compat.v1.set_random_seed(SEED)\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPA:\n",
    "    def __init__(self, num_users, max_sents, max_sent_length, \n",
    "                 embedding_mat, npratio = 4, dense_dim = 200, lr = 0.00025):\n",
    "        self.__num_users = num_users\n",
    "        self.__max_sents = max_sents\n",
    "        self.__max_sent_length = max_sent_length\n",
    "        self.__embedding_mat = embedding_mat\n",
    "        \n",
    "        self.__embedding_dim = embedding_mat.shape[1]\n",
    "        self.__dense_dim = dense_dim\n",
    "        self.__npratio = npratio\n",
    "        \n",
    "        self.__lr = lr\n",
    "        \n",
    "        # User embeddings\n",
    "        self.user_input = Input(shape=(1,), dtype='uint64')\n",
    "        user_embedding_layer = Embedding(self.__num_users, self.__max_sents, trainable=True) #ouput leng == max clicked_news\n",
    "        user_embedding = user_embedding_layer(self.user_input)\n",
    "        \n",
    "        user_embedding_word = Dense(self.__dense_dim, activation='relu')(user_embedding)\n",
    "        user_embedding_word = Flatten()(user_embedding_word)\n",
    "        \n",
    "        user_embedding_news = Dense(self.__dense_dim, activation='relu')(user_embedding)\n",
    "        user_embedding_news = Flatten()(user_embedding_news)\n",
    "        \n",
    "        # thuytt - sử dụng user_embedding cho backend PNRec\n",
    "        self.user_embedd = Model(self.user_input, user_embedding_word)\n",
    "        \n",
    "        # News embedding \n",
    "        self.news_input = Input(shape=(self.__max_sent_length,), dtype='int32')\n",
    "        embedding_layer = Embedding(self.__embedding_mat.shape[0], self.__embedding_dim, \n",
    "                                    weights=[self.__embedding_mat],trainable=True) # ?? Co train hay giu nguyen?\n",
    "        embedded_sequences = embedding_layer(self.news_input)\n",
    "        embedded_sequences =Dropout(0.2)(embedded_sequences)\n",
    "        \n",
    "        cnn_layer = Conv1D(padding='same', activation='relu', strides=1, \n",
    "                          filters = self.__embedding_dim, kernel_size=3)(embedded_sequences)\n",
    "        cnnoutput = Dropout(0.2)(cnn_layer)\n",
    "        \n",
    "        # thuytt - sử dụng để lấy CNNoutput cho articles\n",
    "        self.new_embedding_extractor = Model([self.news_input, self.user_input], cnnoutput)\n",
    "        \n",
    "        attention_a = Dot((2, 1))([cnnoutput, Dense(self.__embedding_dim, activation='tanh')(user_embedding_word)])\n",
    "        attention_weight = Activation('softmax')(attention_a)\n",
    "        news_rep = keras.layers.Dot((1, 1))([cnnoutput, attention_weight])\n",
    "        \n",
    "        self.news_encoder = Model([self.news_input, self.user_input], news_rep)\n",
    "        \n",
    "        # clicked news embedding\n",
    "        clicked_news_input = [keras.Input((self.__max_sent_length,), dtype='int32') for _ in range(MAX_SENTS)]\n",
    "        clicked_news_rep = [self.news_encoder([news, self.user_input]) for news in clicked_news_input]\n",
    "        clicked_news_rep = concatenate([Lambda(lambda x: K.expand_dims(x, axis=1))(news) for news in clicked_news_rep],axis=1)\n",
    "\n",
    "        # User Embedding\n",
    "        attention_news = keras.layers.Dot((2, 1))(\n",
    "            [clicked_news_rep, Dense(self.__embedding_dim, activation='tanh')(user_embedding_news)])\n",
    "        attention_weight_news = Activation('softmax')(attention_news)\n",
    "        user_rep = keras.layers.Dot((1, 1))([clicked_news_rep, attention_weight_news])\n",
    "\n",
    "        # candidate news embedding\n",
    "        candidate_news_input = [keras.Input((self.__max_sent_length,), dtype='int32') for _ in range(1 + self.__npratio)]\n",
    "        candidate_vecs = [self.news_encoder([candidate, self.user_input]) for candidate in candidate_news_input]\n",
    "\n",
    "        # Click probability\n",
    "        logits = [keras.layers.dot([user_rep, candidate_vec], axes=-1) for candidate_vec in candidate_vecs]\n",
    "        logits = keras.layers.Activation(keras.activations.softmax)(keras.layers.concatenate(logits))\n",
    "\n",
    "        self.trainer = Model(candidate_news_input + clicked_news_input + [self.user_input], logits)\n",
    "        self.trainer.compile(loss='categorical_crossentropy', optimizer = Adam(self.__lr), metrics=['acc'])\n",
    "        \n",
    "        candidate_one_vec = self.news_encoder([self.news_input, self.user_input])\n",
    "        candidate_score = Activation(keras.activations.sigmoid)(keras.layers.dot([user_rep, candidate_one_vec], axes=-1))\n",
    "        self.evaluator = Model([self.news_input] + clicked_news_input + [self.user_input], candidate_score)\n",
    "    \n",
    "    def train(self, data_generator, verbose=True):\n",
    "        self.trainer.fit_generator(data_generator, epochs=1, steps_per_epoch=25, verbose=verbose)\n",
    "        \n",
    "    def evaluate(self, data_generator, steps, verbose=True):\n",
    "        self.evaluator.predict_generator(data_generator, steps=steps, verbose=verbose)\n",
    "        \n",
    "    def extract_news_embedding(self, user_id, article):\n",
    "        self.news_embedding_extractor.predict([article, user_id])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-stranger",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "general-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrhistoryt of [auc, mrr, ndcg@5, ndcg@10]\n",
    "best_metric = [0., 0., 0., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "waiting-force",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "25/25 [==============================] - 16s 622ms/step - loss: 1.9722 - acc: 0.2200\n",
      "2/2 [==============================] - 2s 976ms/step\n",
      "Epoch 1/1\n",
      "25/25 [==============================] - 3s 137ms/step - loss: 1.6152 - acc: 0.1800\n",
      "2/2 [==============================] - 0s 55ms/step\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "MAX_SENT_LENGTH=30 # số lượng tokens cho mỗi article\n",
    "MAX_SENTS=50 # maximum clicked article for user embedding\n",
    "batch_size=32\n",
    "n_epoch=100\n",
    "\n",
    "npa = NPA(num_users, MAX_SENTS, MAX_SENT_LENGTH, embedding_mat)\n",
    "\n",
    "for ep in range(2):\n",
    "    traingen = generate_batch_data_train(all_train_pn, all_label, all_train_id, all_user_pos, batch_size)\n",
    "    npa.train(traingen)\n",
    "    testgen=generate_batch_data_test(all_test_pn, all_test_label, all_test_id, all_test_user_pos, batch_size)\n",
    "    click_score = npa.evaluate(testgen, steps=len(all_test_id)//batch_size, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-municipality",
   "metadata": {},
   "source": [
    "## đây là dữ liệu cnnoutput lưu ở DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "educated-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../thuytt_ver2.1/articles_rep10k_raw.pkl', 'rb')\n",
    "articles_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "guilty-footage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.44010243, 0.2914183 ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.01015337, ..., 0.01342217,\n",
       "         0.0008333 , 0.04235733],\n",
       "        [0.        , 0.        , 0.01015337, ..., 0.01342217,\n",
       "         0.0008333 , 0.04235733],\n",
       "        [0.        , 0.        , 0.00506613, ..., 0.00440978,\n",
       "         0.00539138, 0.01715868]]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# đây là 1 CNNoutput\n",
    "example_cnnOutput = articles_test[list(articles_test)[0]]\n",
    "example_cnnOutput"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
